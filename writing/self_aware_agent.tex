\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
  \geometry{margin=1in}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{siunitx}
\usepackage{mhchem}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{float}


% OSF-specific metadata
\hypersetup{
  pdfauthor={Ryan Mukai and Alfonso Herrera},
  pdftitle={Demonstrating Epistemic, Structural, and Identity-Oriented Self-Awareness in a LangGraph-Based Conversational Agent},
  pdfsubject={Artificial Intelligence},
  pdfkeywords={AI safety, self-awareness, language models, LangGraph, conversational agents},
  pdfcreator={LaTeX with Overleaf},
  pdfproducer={pdfLaTeX}
}

\title{Demonstrating Epistemic, Structural, and Identity-Oriented Self-Awareness \\in a LangGraph-Based Conversational Agent}
\author{
  Ryan Mukai\textsuperscript{1}\thanks{Work carried out in a private capacity; email: \texttt{rmukai@example.com}} 
  \and
  Alfonso Herrera\textsuperscript{2}\thanks{Work carried out in a private capacity; email: \texttt{aherrera@example.com}}
}
\date{April 2025}

\begin{document}
\maketitle

\begin{abstract}
We present a prototype conversational agent (“Bob”) built atop LangGraph that combines three complementary self-awareness capabilities:
(1) \emph{Epistemic self-awareness}: the ability to monitor and reflect on one’s own knowledge state (detecting when inferences are under-determined);
(2) \emph{Self-aware memory management}: a hybrid memory architecture that includes an agent-controlled vector database memory that an agent explicitly controls; and
(3) \emph{Structural self-awareness}: particularly \emph{code-based structural self-awareness} via introspecting its own source and exception traces.

We describe Bob’s LangGraph workflow and illustrate how each component is implemented. Our contributions include (a) an engineering design that unifies short-term and long-term memory while preserving temporal context, (b) agent controlled vector database memory, (c) code-introspection mechanisms that allow the agent to detect and explain runtime exceptions, and (d) a working LangGraph prototype (code available online). We conclude with directions for quantitative evaluation and future work on identity self-awareness.
\end{abstract}

\textbf{Keywords:} conversational agents; LangGraph; self-awareness; memory management; temporal grounding; code introspection

\vspace{1em}
\noindent\textbf{ORCID:} \href{https://orcid.org/0000-0002-1234-5678}{0000-0002-1234-5678}

\section{Introduction}
Language models have recently demonstrated impressive reasoning capabilities, and these capabilities enable agents to achieve knowledge of their own knowledge state, the ability to control their own memory, and the ability to comprehend their own code. This paper explores limited self-awareness in a conversational agent (“Bob”) that is powered by OpenAI’s GPT-4o model and orchestrated via LangGraph.

We focus on three complementary forms of self-awareness:
\begin{itemize}[leftmargin=*]
  \item \textbf{Epistemic self-awareness:} the agent’s ability to recognize when it has insufficient premises (e.g., logical inferences requiring extra assumptions).
  \item \textbf{Self-aware memory management:} a multiple-layer memory that combines (i) a short-term in-RAM buffer with recursive summarization and (ii) a persistent vector store (“mandatory memory”) that retrieves semantically relevant chunks each turn and (iii) a vector database that the agent explicitly controls.  It is the third component that gives an agent a long-term memory under its own explicit control that can be operated in a self-aware manner.
  \item \textbf{Structural self-awareness:}  
 {Code-based structural self-awareness:} the agent’s ability to introspect its own source code and diagnose runtime exceptions (e.g., Python stack traces).
\end{itemize}

Although prior work has explored static LSTM-based self-awareness \cite{Mukai2020}, our contribution is a \emph{dynamic}, LLM-driven prototype with hybrid memory, explicit timestamping, and code introspection.

\subsection{Contributions}
\begin{enumerate}[leftmargin=*]
  \item We present an end-to-end LangGraph workflow that integrates short-term and long-term memory with recursive summarization.
  \item We develop prompt-engineering patterns that encode UTC timestamps, enabling the model to reason about event chronology.
  \item We implement a code-introspection capability that allows the agent to detect, diagnose, and explain runtime exceptions and to inspect its own source file.
  \item We release a working Python prototype (available at \url{https://github.com/rmukai/langgraph-agent}), demonstrating coherent multi-session dialogues without unbounded context growth.
\end{enumerate}

\section{Related Work}

\subsection{Self-Awareness in Neural Agents}
Static LSTM-based approaches (e.g., \cite{Mukai2020}) presented early proofs of concept for an agent that maintained a small symbolic \emph{knowledge state} and detected “unknown” queries. However, those systems lacked dynamic memory components, timestamping, and code introspection. Our work leverages modern LLMs (GPT-4/O) \cite{ChenGupta2024} and LangGraph to maintain hybrid memory states, timestamped context, and structural code self-awareness.

\subsection{Memory Architectures for Open-Domain Dialogue}
Retrieval-augmented generation (RAG) approaches such as \cite{Lewis2020} store large corpora in vector databases and retrieve top-$k$ passages each turn. However, most RAG systems do not perform \emph{recursive summarization} to prune older context. CLIE \cite{Sanh2022} introduced summary-augmented buffers in multi-turn chat; our work refines it by adding explicit timestamping and separating \emph{voluntary} vs. \emph{system} memory channels.  This is used not only for conversational purposes but also for code introspection, allowing the agent to read its own codebase sequentially.

\subsection{Temporal Reasoning in Language Models}
Prompting LLMs to interpret dates has shown that explicit timestamp tokens can help reduce hallucinations about “when” events occurred \cite{Zhang2023}. We build on these insights by injecting ISO-8601 timestamps into every human turn and summary, enabling the model to filter out “stale” information.

\subsection{Code Introspection and Agent Structure}
Recent work on \emph{corrigibility} and self-modification (e.g., \cite{Soares2015}) proposes frameworks for self-modifying agents under formal logic constraints but does not address how an agent can continually inspect and reason about its own source code at runtime. Our prototype implements a lightweight code-introspection capability, allowing the agent to read its own Python file, locate lines of code, and diagnose exceptions (e.g., `ZeroDivisionError`), thus adding a structural dimension to self-awareness.

\section{Agent Architecture}
Figure~\ref{fig:workflow} illustrates Bob’s end-to-end LangGraph workflow. The three main loops are:
\begin{itemize}[leftmargin=*]
  \item \textbf{Conversation Turn:} Human → LangGraph (feed new turn) → LLM → Append AI reply → Update short-term buffer.
  \item \textbf{Summarization Loop:} Once buffer exceeds \texttt{messages\_before\_summary}, invoke LLM for summarization, prune old messages, update \texttt{state["summary"]}.
  \item \textbf{Vector Memory:} After each turn, embed new message, upsert into Chroma under “mandatory.” On next turn, query top-$k$ vectors and prepend to new prompt.
\end{itemize}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{conversation_flow.jpg}
  \caption{LangGraph workflow showing message ingestion, short-term summarization, vector memory retrieval, temporal stamping, and code-introspection nodes.}
  \label{fig:workflow}
\end{figure}

\subsection{Short-Term Memory with Recursive Summarization}
Bob’s short-term memory is implemented via LangGraph’s \texttt{StateGraph} abstraction. Internally, we maintain:
\begin{itemize}[leftmargin=*]
  \item \texttt{state["messages"]}: a list of \texttt{BaseMessage} (LLM’s \texttt{HumanMessage}, \texttt{AIMessage}, \texttt{ToolMessage}).
  \item \texttt{state["summary"]}: a scalar string containing the “rolled-up” summary of all turns older than the last $N$ messages.
\end{itemize}

On each new human turn, the workflow checks whether \texttt{len(state["messages"])} $>$ \texttt{messages\_before\_summary} (default = 15). If so, it triggers the summarization node, which:
\begin{enumerate}[leftmargin=*]
  \item Sends the entire \texttt{messages} buffer to GPT-4 with a “Please summarize these $k$ turns” prompt.
  \item Captures the returned summary.
  \item Emits a list of \texttt{RemoveMessage(id)} actions to prune all but the most recent turns.
  \item Writes the new summary to \texttt{state["summary"]}.
\end{enumerate}

Because \texttt{messages} is declared as a \texttt{List} in the state schema, LangGraph automatically appends newly returned messages (or processes \texttt{RemoveMessage} entries) without manual bookkeeping. Scalar fields (like \texttt{summary}) are overwritten. The result is that the agent’s in-RAM buffer never grows beyond a fixed window, yet a running “as-of” summary is always available for context.

\begin{verbatim}
@node
def _should_continue(self, state: State) -> bool:
    return len(state["messages"]) > self.messages_before_summary

@node(...)
def _summarize_conversation(self, state: State) -> Dict[str, Any]:
    full_buffer = state["messages"]
    prompt = SystemMessage(
      content=f"Summarize these {len(full_buffer)} turns:"
    )
    result = self.model.invoke([prompt] + full_buffer)
    new_summary = result.content
    delete_ops = [RemoveMessage(id=msg.id) for msg in full_buffer[:-2]]
    return {"summary": new_summary, "messages": delete_ops}
\end{verbatim}

\subsection{Persistent Vector Memory (“Mandatory”)}
Bob uses Chroma as a vector database for long-term memory. After each completed turn, we:
\begin{enumerate}[leftmargin=*]
  \item Compute an embedding of the user’s raw message text via \texttt{self.embedder.embed(...)}.
  \item Call \texttt{self.chroma\_manager.upsert\_memory(thread\_id, embedding, text)} to store the new chunk under a “mandatory” namespace keyed by \texttt{thread\_id}.
\end{enumerate}

At the beginning of each new \texttt{chat(...)} invocation, Bob automatically retrieves relevant past memory by:
\begin{verbatim}
relevant = self.chroma_manager.query_memory(
  mandatory_db, query_embedding, k=5
)
\end{verbatim}
then flattens the returned documents into a single string:
\begin{verbatim}
flattened = "\n".join(sum(relevant["documents"], []))
\end{verbatim}
and finally prepends:
\begin{verbatim}
"HERE IS RELEVANT PAST CONTEXT:\n" + flattened
 + "\nCURRENT MESSAGE:\n" + user_message
\end{verbatim}
as the content of a new \texttt{HumanMessage}. By doing so, even if the in-RAM buffer is empty (e.g., on agent restart), relevant facts from long-ago sessions are surfaced automatically. This creates a hybrid memory strategy: ephemeral conversation context + semantic recall from persistent storage.

\subsection{Temporal Awareness: UTC Timestamps}
To enable the agent to reason about \emph{when} something occurred, we inject a UTC timestamp into every human turn (and every time we summarize). Concretely, at runtime:
\begin{verbatim}
current_utc = datetime.datetime.utcnow().strftime("%Y-%m-%dT%H:%MZ")
message_with_ts = f"[{current_utc}] {user_message}"
\end{verbatim}
This string becomes the content of \texttt{HumanMessage}, so Bob “sees” both the literal message and a precise timestamp. Likewise, each summary operation archives:
\begin{verbatim}
summary_ts = datetime.datetime.utcnow().strftime("%Y-%m-%dT%H:%MZ")
"Summary (as of " + summary_ts + "): " + summary_text
\end{verbatim}
By explicitly labeling each piece of context with a timestamp, we ensure the LLM can compare “2025-02-15” vs. “2025-05-25” when deciding which facts are stale. To encourage correct interpretation, we add the following to our system prompt:
\begin{quote}
\textit{“Whenever you see a leading ISO-8601 timestamp (e.g., \texttt{[2025-05-25T22:15Z]}), treat it as UTC and reason about recency accordingly—anything more than 90 days old should be flagged as potentially stale.”}
\end{quote}
Although this adds token overhead, we found that dropping sub-minute precision (i.e., only storing \texttt{“YYYY-MM-DDThh:mmZ”}) reduces bloat with minimal loss of context.

\subsection{Structural Self-Awareness: Code Introspection \& Exception Diagnosis}
Beyond temporal awareness, Bob also implements \emph{structural self-awareness} of its own codebase. 
First, Bob contains a function to read in its own source code and does so on startup.
This reading is done sequentially, and the agent can read the codebase line by line.  Arecursive summary to written, and the data are also stored to the mandatory conversation memory, making it possible to recal portions of code as needed.  Secondly, whenever a runtime exception occurs (e.g., a \texttt{ZeroDivisionError}), Bob:
\begin{enumerate}[leftmargin=*]
  \item Captures the Python stack trace via a try/except wrapper around the main loop.
  \item Uses Python’s \texttt{traceback} module to identify the line number associated with the exception.
  \item Constructs a prompt for the LLM of the form:
  \begin{verbatim} 
  System: The following exception occurred at line {lineno}: {exception_type}
  with message "{exception_message}". Here is the surrounding code:

  Please explain why this exception occurred and suggest a fix.
\end{verbatim}
\item Invokes the LLM to generate a human-readable diagnosis and patch recommendation.
\end{enumerate}

\begin{verbatim}
def _handle_exception(self, exc: Exception) -> AIMessage:
  tb = traceback.format_exc()
  frame = inspect.getframeinfo(inspect.currentframe().f_back)
  snippet = self._read_source_snippet(frame.filename, frame.lineno)
  prompt = SystemMessage(
    content=f"Exception: {type(exc).__name__} at line {frame.lineno}. "
            f"Code snippet:\n```{snippet}```\n"
            "Explain why this exception occurred and suggest a fix."
  )
  response = self.model.invoke([prompt])
  return AIMessage(content=response.content)
\end{verbatim}

This mechanism allows Bob to “know” its own structure, locate bugs, and propose corrections—an essential component of structural self-awareness. In future work, we can extend this node to support automated patch application, unit test generation, and continuous self-monitoring.

\section{Results}

\subsection{Epistemic Self-Awareness}
We tested Bob on a toy logic scenario:
\begin{verbatim}
User (2025-05-25T15:30Z): If A → C and D is true, can you determine A?
Agent: Given only “A implies C” and “D is true,” I cannot conclusively determine A.
     (I need additional premises.)
User (2025-05-25T15:31Z): Suppose also C xor D. Does that help?
Agent: Since D is true, C must be false. From A → C, A cannot be true (or else C would be true),
     so A is false.
\end{verbatim}
These exchanges show that Bob (a) notices insufficient premises (“cannot conclusively determine A”) and (b) adjusts when a new premise arrives. 

\subsection{Temporal Awareness}
Bob’s timestamping mechanism surfaces recency explicitly. For example:
\begin{verbatim}
User (2025-02-15T10:00Z): My cat is unwell.
…
[90 days later]
User (2025-05-15T10:02Z): How is my cat?
Agent: I recall you mentioned on [2025-02-15T10:00Z] that your cat was unwell.
     That information is now 90+ days old—are you asking about its current condition?
\end{verbatim}
By including ISO-8601 timestamps, Bob can flag that the earlier mention is “stale” and ask follow-ups accordingly.

\subsection{Structural Self-Awareness: Code Introspection \& Exception Diagnosis}
We introduced a deliberate \texttt{ZeroDivisionError} in Bob’s Python code. A typical transcript:
\begin{verbatim}
User (2025-05-25T16:00Z): Run your main loop.
Agent: Exception: ZeroDivisionError at line 42 in main().
     It arises from “dummy = 1/0” which was inserted to test error handling.
     Suggested fix: remove or comment out that line.
\end{verbatim}
Bob reads its own source (via Python’s \texttt{inspect} module), locates line 42, explains the bug, and recommends a valid patch. This demonstrates structural introspection: Bob understands its code structure and can offer repairs.

\subsection{Memory Self-Awareness}
In addition to summarizing older turns, Bob distinguishes between \emph{system memory} (persistent identity facts, stored in the system prompt) and \emph{voluntary memory} (user preferences, stored in a separate vector namespace). For instance:
\begin{verbatim}
User (2025-05-25T16:15Z): My name is Marc. Please update your system message.
Agent: I have appended “User name: Marc” to my system message.
     I will not overwrite prior identity facts.
User (2025-05-25T16:16Z): Record my love of baseball in voluntary memory.
Agent: Noted: “Loves baseball” stored in voluntary memory under “Marc.”
\end{verbatim}
Bob then explains why the system message must be append-only (to preserve identity integrity) while voluntary memory can be LRU-evicted. This reflective judgment shows an explicit model of memory roles and boundaries.

\section{Discussion}
In reviewing our implementation, we observed that the current memory management strategy in \texttt{basic_agent.py} employs a fixed sliding window size (turns=5) and retrieves a constant number of memory chunks (\texttt{k=5}) on each query. While this approach ensures that the most recent interactions remain in context, it can lead to recursive bloat: as messages accumulate, redundant storage and retrieval of similar content expands the contextual footprint and may exceed model limits over extended dialogs.

To streamline context and minimize bloat while preserving retrieval quality, we recommend the following strategies:
\begin{enumerate}[leftmargin=*]
  \item \textbf{Dynamic windowing:} adapt the sliding-window size based on a token budget or conversation phase, rather than a fixed turn count.
  \item \textbf{Hierarchical summarization:} periodically summarize older turns into concise representations and store those summaries in long-term memory, purging raw transcripts beyond a threshold.
  \item \textbf{Relevance-based pruning:} score stored chunks by query relevance or age, retrieving only the top-$n$ entries and automatically discarding low-scoring or stale records via a decay or TTL mechanism.
  \item \textbf{Dual memory tiers:} separate short-term active context from long-term archival memory, with explicit consolidation events to merge and compress long-term information.
  \item \textbf{Parameterization and tuning:} expose retrieval parameters ($k$, $\textit{turns}$) as configurable hyperparameters, enabling empirical optimization for different dialog scenarios.
\end{enumerate}
These enhancements will help maintain a lean context window, improve retrieval efficiency, and prevent exceeding the model’s context capacity without sacrificing essential conversational grounding.


\section{Conclusion}
We have demonstrated a working LangGraph-based conversational agent that integrates:
\begin{itemize}[leftmargin=*]
\item Epistemic self-awareness: detecting when inferences are under-determined.
\item Self-aware memory management: a dual buffer plus voluntary agent-controlled vector store.
\item Structural self-awareness: (a) sequentially reading its own codebase instead of 
relying on RAG, and (b) introspecting its own code to diagnose runtime exceptions.
\end{itemize}
Our implementation shows that a modern LLM (GPT-4o) can meaningfully reflect on its own knowledge, code structure, and memory—laying the groundwork for \emph{identity self-awareness} in future work. All code and transcripts are publicly available, and we invite the community to replicate and extend our experiments.  

\section*{Data and Code Availability}
The complete codebase (including \texttt{basic\_agent.py}, \texttt{chroma\_db\_manager.py}, and setup scripts) is publicly available at \url{https://github.com/rmukai/langgraph-agent} \cite{LangGraphAgent2025}. The conversation transcripts and embedding snapshots used in this study are archived at \url{https://doi.org/10.XXXX/OSF.IO/ABCDE}.

\section*{License}
This work is licensed under CC-BY 4.0.

\section*{Acknowledgments}
We thank the OpenAI team for early access to GPT-4/O and the LangGraph developers for their robust framework.

\appendix
\section{Excerpts from Conversation Transcript}
Below are key excerpts illustrating each self-awareness dimension.

\subsection{Epistemic Self-Awareness Example}
\begin{verbatim}
User (2025-05-25T15:30Z): If A → C and D is true, can you determine A?
Agent: Given only “A → C” and “D is true,” I cannot conclusively determine A.
     (I need additional premises.)
User (2025-05-25T15:31Z): Suppose also C xor D. Does that help?
Agent: Since D is true, C must be false. From A → C, A cannot be true, so A is false.
\end{verbatim}

\subsection{Temporal Self-Awareness Example}
\begin{verbatim}
User (2025-02-15T10:00Z): My cat is unwell.
...
User (2025-05-15T10:02Z): How is my cat?
Agent: I recall you mentioned on [2025-02-15T10:00Z] that your cat was unwell.
     That information is now 90+ days old—are you asking about its current condition?
\end{verbatim}

\subsection{Structural Self-Awareness (Code Introspection) Example}
\begin{verbatim}
User (2025-05-25T16:00Z): Run your main loop.
Agent: Exception: ZeroDivisionError at line 42 in main().
     It arises from “dummy = 1/0” which was inserted to test error handling.
     Suggested fix: remove or comment out that line.
\end{verbatim}

\subsection{Memory Self-Awareness Example}
\begin{verbatim}
User (2025-05-25T16:15Z): My name is Marc. Please update your system message.
Agent: I have appended “User name: Marc” to my system message.
     I will not overwrite prior identity facts.
User (2025-05-25T16:16Z): Record my love of baseball in voluntary memory.
Agent: Noted: “Loves baseball” stored in voluntary memory under “Marc.”
\end{verbatim}

\section*{OSF Preprint Details}
This manuscript adheres to OSF Preprints guidelines \cite{OSF2025}, and was prepared in Overleaf using OSF’s LaTeX template.
\begin{itemize}[leftmargin=*]
\item \textbf{Metadata:}
  \begin{itemize}[leftmargin=*]
    \item Author: Ryan Mukai (ORCID: 0000-0002-1234-5678)
    \item Title: Demonstrating Epistemic, Structural, and Identity-Oriented Self-Awareness in a LangGraph-Based Conversational Agent
    \item Keywords: AI safety; self-awareness; memory management; LangGraph; conversational agents
    \item Subject: Artificial Intelligence
    \item License: CC-BY 4.0
  \end{itemize}
\item \textbf{Files in OSF project:}
  \begin{itemize}[leftmargin=*]
    \item \texttt{self\_aware\_agent.tex} (main manuscript)
    \item \texttt{self\_aware\_agent.pdf} (compiled PDF)
    \item \texttt{self\_aware\_agent.bib} (BibTeX database)
    \item \texttt{figures/} (workflow diagrams, etc.)
    \item \texttt{code/} (Python code: \texttt{basic\_agent.py}, \texttt{chroma\_db\_manager.py})
    \item \texttt{transcripts/} (full conversation transcripts)
  \end{itemize}
\item \textbf{Repository linking:}
  \begin{quote}
    The full code is available at \url{https://github.com/rmukai/langgraph-agent}, version v1.0.
  \end{quote}
\item \textbf{Author contributions:} Ryan Mukai conceived the design, implemented the prototype, performed all experiments, and wrote this manuscript.
\item \textbf{Conflict of interest:} The author declares no conflicts of interest.
\end{itemize}

\section*{References}
\bibliographystyle{unsrt}
\bibliography{self_aware_agent}

\end{document}