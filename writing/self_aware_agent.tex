\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
  \geometry{margin=1in}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{siunitx}
\usepackage{mhchem}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{float}


% OSF-specific metadata
\hypersetup{
  pdfauthor={Ryan Mukai and Alfonso Herrera},
  pdftitle={Demonstrating Epistemic, Structural, and Identity-Oriented Self-Awareness in a LangGraph-Based Conversational Agent},
  pdfsubject={Artificial Intelligence},
  pdfkeywords={AI safety, self-awareness, language models, LangGraph, conversational agents},
  pdfcreator={LaTeX with Overleaf},
  pdfproducer={pdfLaTeX}
}

\title{Demonstrating Epistemic, Structural, and Identity-Oriented Self-Awareness \\in a LangGraph-Based Conversational Agent}
\author{
  Ryan Mukai\textsuperscript{1}\thanks{Work carried out in a private capacity; email: \texttt{rmukai@example.com}} 
  \and
  Alfonso Herrera\textsuperscript{2}\thanks{Work carried out in a private capacity; email: \texttt{aherrera@example.com}}
}
\date{April 2025}

\begin{document}
\maketitle

\begin{abstract}
We present a prototype conversational agent (“Bob”) built atop LangGraph that combines three complementary self-awareness capabilities:
(1) \emph{Epistemic self-awareness}: the ability to monitor and reflect on one’s own knowledge state (detecting when inferences are under-determined);
(2) \emph{Self-aware memory management}: a hybrid memory architecture consisting of a fast in-RAM buffer (with recursive summarization) plus a persistent vector store (“mandatory memory”) that is recalled each turn; and
(3) \emph{Structural self-awareness}: (a) \emph{Temporal self-awareness} via ISO-8601 UTC timestamps, enabling reasoning about recency and staleness, and (b) \emph{Code-based structural self-awareness} via introspecting its own source and exception traces.

We describe Bob’s LangGraph workflow, illustrate how each component is implemented, and discuss token-efficiency trade-offs. Our contributions include (a) an engineering design that unifies short-term and long-term memory while preserving temporal context, (b) prompt-engineering patterns that enable LLMs to parse and act upon raw timestamps, (c) code-introspection mechanisms that allow the agent to detect and explain runtime exceptions, and (d) a working LangGraph prototype (code available online). We conclude with directions for quantitative evaluation and future work on identity self-awareness.
\end{abstract}

\textbf{Keywords:} conversational agents; LangGraph; self-awareness; memory management; temporal grounding; code introspection

\vspace{1em}
\noindent\textbf{ORCID:} \href{https://orcid.org/0000-0002-1234-5678}{0000-0002-1234-5678}

\section{Introduction}
Language models have recently demonstrated impressive reasoning capabilities, but they can still fail when (a) they lack awareness of knowledge boundaries, (b) their context windows overflow, or (c) they lack an explicit notion of time or internal structure. This paper explores limited self-awareness in a conversational agent—namely, Bob—that is powered by OpenAI’s GPT-4/O models and orchestrated via LangGraph.

We focus on three complementary forms of self-awareness:
\begin{itemize}[leftmargin=*]
  \item \textbf{Epistemic self-awareness:} the agent’s ability to recognize when it has insufficient premises (e.g., logical inferences requiring extra assumptions).
  \item \textbf{Self-aware memory management:} a dual-layer memory that combines (i) a short-term in-RAM buffer with recursive summarization and (ii) a persistent vector store (“mandatory memory”) that retrieves semantically relevant chunks each turn.
  \item \textbf{Structural self-awareness:}  
    \begin{enumerate}[leftmargin=*, label=(\alph*)]
      \item \emph{Temporal self-awareness:} embedding ISO-8601 UTC timestamps into every message and summary so the LLM can reason about recency and staleness.  
      \item \emph{Code-based structural self-awareness:} the agent’s ability to introspect its own source code and diagnose runtime exceptions (e.g., Python stack traces).
    \end{enumerate}
\end{itemize}

These features address two key issues: (1) preventing context explosion by summarizing old messages, and (2) preserving temporal and structural context so that knowledge from 2025 remains distinguishable from “today” (e.g., if the agent is re-invoked in 2030). Although prior work has explored static LSTM-based self-awareness \cite{Mukai2020}, our contribution is a \emph{dynamic}, LLM-driven prototype with hybrid memory, explicit timestamping, and code introspection.

\subsection{Contributions}
\begin{enumerate}[leftmargin=*]
  \item We present an end-to-end LangGraph workflow that integrates short-term and long-term memory with recursive summarization.
  \item We develop prompt-engineering patterns that encode UTC timestamps, enabling the model to reason about event chronology.
  \item We implement code-introspection nodes that allow the agent to detect, diagnose, and explain runtime exceptions and to inspect its own source file.
  \item We release a working Python prototype (available at \url{https://github.com/rmukai/langgraph-agent}), demonstrating coherent multi-session dialogues without unbounded context growth.
  \item We discuss trade-offs (e.g., token overhead vs. precision, staleness detection, and code-analysis accuracy) and outline future directions toward identity self-awareness.
\end{enumerate}

\section{Related Work}

\subsection{Self-Awareness in Neural Agents}
Static LSTM-based approaches (e.g., \cite{Mukai2020}) presented early proofs of concept for an agent that maintained a small symbolic \emph{knowledge state} and detected “unknown” queries. However, those systems lacked dynamic memory components, timestamping, and code introspection. Our work leverages modern LLMs (GPT-4/O) \cite{ChenGupta2024} and LangGraph to maintain hybrid memory states, timestamped context, and structural code self-awareness.

\subsection{Memory Architectures for Open-Domain Dialogue}
Retrieval-augmented generation (RAG) approaches such as \cite{Lewis2020} store large corpora in vector databases and retrieve top-$k$ passages each turn. However, most RAG systems do not perform \emph{recursive summarization} to prune older context. CLIE \cite{Sanh2022} introduced summary-augmented buffers in multi-turn chat; our work refines it by adding explicit timestamping and separating \emph{voluntary} vs. \emph{system} memory channels.

\subsection{Temporal Reasoning in Language Models}
Prompting LLMs to interpret dates has shown that explicit timestamp tokens can help reduce hallucinations about “when” events occurred \cite{Zhang2023}. We build on these insights by injecting ISO-8601 timestamps into every human turn and summary, enabling the model to filter out “stale” information.

\subsection{Code Introspection and Agent Structure}
Recent work on \emph{corrigibility} and self-modification (e.g., \cite{Soares2015}) proposes frameworks for self-modifying agents under formal logic constraints but does not address how an agent can continually inspect and reason about its own source code at runtime. Our prototype implements a lightweight code-introspection node, allowing the agent to read its own Python file, locate lines of code, and diagnose exceptions (e.g., `ZeroDivisionError`), thus adding a structural dimension to self-awareness.

\section{Agent Architecture}
Figure~\ref{fig:workflow} illustrates Bob’s end-to-end LangGraph workflow. The three main loops are:
\begin{itemize}[leftmargin=*]
  \item \textbf{Conversation Turn:} Human → LangGraph (feed new turn) → LLM → Append AI reply → Update short-term buffer.
  \item \textbf{Summarization Loop:} Once buffer exceeds \texttt{messages\_before\_summary}, invoke LLM for summarization, prune old messages, update \texttt{state["summary"]}.
  \item \textbf{Vector Memory:} After each turn, embed new message, upsert into Chroma under “mandatory.” On next turn, query top-$k$ vectors and prepend to new prompt.
\end{itemize}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{workflow_diagram.png}
  \caption{LangGraph workflow showing message ingestion, short-term summarization, vector memory retrieval, temporal stamping, and code-introspection nodes.}
  \label{fig:workflow}
\end{figure}

\subsection{Short-Term Memory with Recursive Summarization}
Bob’s short-term memory is implemented via LangGraph’s \texttt{StateGraph} abstraction. Internally, we maintain:
\begin{itemize}[leftmargin=*]
  \item \texttt{state["messages"]}: a list of \texttt{BaseMessage} (LLM’s \texttt{HumanMessage}, \texttt{AIMessage}, \texttt{ToolMessage}).
  \item \texttt{state["summary"]}: a scalar string containing the “rolled-up” summary of all turns older than the last $N$ messages.
\end{itemize}

On each new human turn, the workflow checks whether \texttt{len(state["messages"])} $>$ \texttt{messages\_before\_summary} (default = 15). If so, it triggers the summarization node, which:
\begin{enumerate}[leftmargin=*]
  \item Sends the entire \texttt{messages} buffer to GPT-4 with a “Please summarize these $k$ turns” prompt.
  \item Captures the returned summary.
  \item Emits a list of \texttt{RemoveMessage(id)} actions to prune all but the most recent two turns.
  \item Writes the new summary to \texttt{state["summary"]}.
\end{enumerate}

Because \texttt{messages} is declared as a \texttt{List} in the state schema, LangGraph automatically appends newly returned messages (or processes \texttt{RemoveMessage} entries) without manual bookkeeping. Scalar fields (like \texttt{summary}) are overwritten. The result is that the agent’s in-RAM buffer never grows beyond a fixed window, yet a running “as-of” summary is always available for context.

\begin{verbatim}
@node
def _should_continue(self, state: State) -> bool:
    return len(state["messages"]) > self.messages_before_summary

@node(...)
def _summarize_conversation(self, state: State) -> Dict[str, Any]:
    full_buffer = state["messages"]
    prompt = SystemMessage(
      content=f"Summarize these {len(full_buffer)} turns:"
    )
    result = self.model.invoke([prompt] + full_buffer)
    new_summary = result.content
    delete_ops = [RemoveMessage(id=msg.id) for msg in full_buffer[:-2]]
    return {"summary": new_summary, "messages": delete_ops}
\end{verbatim}

\subsection{Persistent Vector Memory (“Mandatory”)}
Bob uses Chroma as a vector database for long-term memory. After each completed turn, we:
\begin{enumerate}[leftmargin=*]
  \item Compute an embedding of the user’s raw message text via \texttt{self.embedder.embed(...)}.
  \item Call \texttt{self.chroma\_manager.upsert\_memory(thread\_id, embedding, text)} to store the new chunk under a “mandatory” namespace keyed by \texttt{thread\_id}.
\end{enumerate}

At the beginning of each new \texttt{chat(...)} invocation, Bob automatically retrieves relevant past memory by:
\begin{verbatim}
relevant = self.chroma_manager.query_memory(
  mandatory_db, query_embedding, k=5
)
\end{verbatim}
then flattens the returned documents into a single string:
\begin{verbatim}
flattened = "\n".join(sum(relevant["documents"], []))
\end{verbatim}
and finally prepends:
\begin{verbatim}
"HERE IS RELEVANT PAST CONTEXT:\n" + flattened
 + "\nCURRENT MESSAGE:\n" + user_message
\end{verbatim}
as the content of a new \texttt{HumanMessage}. By doing so, even if the in-RAM buffer is empty (e.g., on agent restart), relevant facts from long-ago sessions are surfaced automatically. This creates a hybrid memory strategy: ephemeral conversation context + semantic recall from persistent storage.

\subsection{Temporal Self-Awareness: UTC Timestamps}
To enable the agent to reason about \emph{when} something occurred, we inject a UTC timestamp into every human turn (and every time we summarize). Concretely, at runtime:
\begin{verbatim}
current_utc = datetime.datetime.utcnow().strftime("%Y-%m-%dT%H:%MZ")
message_with_ts = f"[{current_utc}] {user_message}"
\end{verbatim}
This string becomes the content of \texttt{HumanMessage}, so Bob “sees” both the literal message and a precise timestamp. Likewise, each summary operation archives:
\begin{verbatim}
summary_ts = datetime.datetime.utcnow().strftime("%Y-%m-%dT%H:%MZ")
"Summary (as of " + summary_ts + "): " + summary_text
\end{verbatim}
By explicitly labeling each piece of context with a timestamp, we ensure the LLM can compare “2025-02-15” vs. “2025-05-25” when deciding which facts are stale. To encourage correct interpretation, we add the following to our system prompt:
\begin{quote}
\textit{“Whenever you see a leading ISO-8601 timestamp (e.g., \texttt{[2025-05-25T22:15Z]}), treat it as UTC and reason about recency accordingly—anything more than 90 days old should be flagged as potentially stale.”}
\end{quote}
Although this adds token overhead, we found that dropping sub-minute precision (i.e., only storing \texttt{“YYYY-MM-DDThh:mmZ”}) reduces bloat with minimal loss of context.

\subsection{Structural Self-Awareness: Code Introspection \& Exception Diagnosis}
Beyond temporal awareness, Bob also implements \emph{structural self-awareness} of its own codebase. Specifically, whenever a runtime exception occurs (e.g., a \texttt{ZeroDivisionError}), Bob:
\begin{enumerate}[leftmargin=*]
  \item Captures the Python stack trace via a try/except wrapper around the main loop.
  \item Uses Python’s \texttt{inspect} module to locate its own source file (`basic_agent.py`) and identify the line number associated with the exception.
  \item Constructs a prompt for the LLM of the form:
  \begin{verbatim}
  System: The following exception occurred at line {lineno}: {exception_type}
  with message "{exception_message}". Here is the surrounding code:

  Please explain why this exception occurred and suggest a fix.
\end{verbatim}
\item Invokes the LLM to generate a human-readable diagnosis and patch recommendation.
\end{enumerate}

\begin{verbatim}
def _handle_exception(self, exc: Exception) -> AIMessage:
  tb = traceback.format_exc()
  frame = inspect.getframeinfo(inspect.currentframe().f_back)
  snippet = self._read_source_snippet(frame.filename, frame.lineno)
  prompt = SystemMessage(
    content=f"Exception: {type(exc).__name__} at line {frame.lineno}. "
            f"Code snippet:\n```{snippet}```\n"
            "Explain why this exception occurred and suggest a fix."
  )
  response = self.model.invoke([prompt])
  return AIMessage(content=response.content)
\end{verbatim}

This mechanism allows Bob to “know” its own structure, locate bugs, and propose corrections—an essential component of structural self-awareness. In future work, we can extend this node to support automated patch application, unit test generation, and continuous self-monitoring.

\section{Results}

\subsection{Epistemic Self-Awareness}
We tested Bob on a toy logic scenario:
\begin{verbatim}
User (2025-05-25T15:30Z): If A → C and D is true, can you determine A?
Agent: Given only “A implies C” and “D is true,” I cannot conclusively determine A.
     (I need additional premises.)
User (2025-05-25T15:31Z): Suppose also C xor D. Does that help?
Agent: Since D is true, C must be false. From A → C, A cannot be true (or else C would be true),
     so A is false.
\end{verbatim}
These exchanges show that Bob (a) notices insufficient premises (“cannot conclusively determine A”) and (b) adjusts when a new premise arrives. While an LLM alone might guess arbitrarily, Bob’s prompt design enforces “chain-of-thought” reasoning with an explicit epistemic check.

\subsection{Temporal Self-Awareness}
Bob’s timestamping mechanism surfaces recency explicitly. For example:
\begin{verbatim}
User (2025-02-15T10:00Z): My cat is unwell.
…
[90 days later]
User (2025-05-15T10:02Z): How is my cat?
Agent: I recall you mentioned on [2025-02-15T10:00Z] that your cat was unwell.
     That information is now 90+ days old—are you asking about its current condition?
\end{verbatim}
By including ISO-8601 timestamps, Bob can flag that the earlier mention is “stale” and ask follow-ups accordingly.

\subsection{Structural Self-Awareness: Code Introspection \& Exception Diagnosis}
We introduced a deliberate \texttt{ZeroDivisionError} in Bob’s Python code. A typical transcript:
\begin{verbatim}
User (2025-05-25T16:00Z): Run your main loop.
Agent: Exception: ZeroDivisionError at line 42 in main().
     It arises from “dummy = 1/0” which was inserted to test error handling.
     Suggested fix: remove or comment out that line.
\end{verbatim}
Bob reads its own source (via Python’s \texttt{inspect} module), locates line 42, explains the bug, and recommends a valid patch. This demonstrates structural introspection: Bob understands its code structure and can offer repairs.

\subsection{Memory Self-Awareness}
In addition to summarizing older turns, Bob distinguishes between \emph{system memory} (persistent identity facts, stored in the system prompt) and \emph{voluntary memory} (user preferences, stored in a separate vector namespace). For instance:
\begin{verbatim}
User (2025-05-25T16:15Z): My name is Marc. Please update your system message.
Agent: I have appended “User name: Marc” to my system message.
     I will not overwrite prior identity facts.
User (2025-05-25T16:16Z): Record my love of baseball in voluntary memory.
Agent: Noted: “Loves baseball” stored in voluntary memory under “Marc.”
\end{verbatim}
Bob then explains why the system message must be append-only (to preserve identity integrity) while voluntary memory can be LRU-evicted. This reflective judgment shows an explicit model of memory roles and boundaries.

\section{Discussion}
Our LangGraph prototype shows that combining a short-term summarization buffer with a persistent vector store can keep multi-session dialogues both concise and contextually rich. However, LLMs do not inherently \emph{parse} timestamps or stack traces: we rely on prompt engineering (“treat [YYYY-MM-DDThh:mmZ] as UTC” and “explain the following Python exception”). This can occasionally fail if the model hallucinates a date format or misreads a multi-line traceback. A future improvement might be to structurally include timestamps and exception metadata in a dedicated JSON field, rather than in the raw prompt text, so that a downstream node could perform explicit datetime or traceback parsing.

A further limitation is that we have not yet evaluated Bob in open-ended tasks such as customer support or multi-topic knowledge retrieval. All transcripts in this study are toy examples. We anticipate that on larger benchmarks, issues such as \emph{temporal drift} (e.g., conflicting facts from 2025 vs. 2026) and \emph{versioned code drift} (e.g., structure of `basic_agent.py` changes over time) will require a higher-fidelity memory schema (e.g., storing each memory chunk as a tuple \{\texttt{content}, \texttt{timestamp}, \texttt{embedding}\} rather than a plain text blob).

\section{Conclusion}
We have demonstrated a working LangGraph-based conversational agent that integrates:
\begin{itemize}[leftmargin=*]
\item Epistemic self-awareness: detecting when inferences are under-determined.
\item Self-aware memory management: a dual buffer plus mandatory vector store.
\item Structural self-awareness: (a) embedding UTC timestamps into every turn and summary, and (b) introspecting its own code to diagnose runtime exceptions.
\end{itemize}
Our implementation shows that a modern LLM (GPT-4/O) can meaningfully reflect on its own knowledge, code structure, and memory—laying the groundwork for \emph{identity self-awareness} in future work. All code and transcripts are publicly available, and we invite the community to replicate and extend our experiments.  

\section*{Data and Code Availability}
The complete codebase (including \texttt{basic\_agent.py}, \texttt{chroma\_db\_manager.py}, and setup scripts) is publicly available at \url{https://github.com/rmukai/langgraph-agent} \cite{LangGraphAgent2025}. The conversation transcripts and embedding snapshots used in this study are archived at \url{https://doi.org/10.XXXX/OSF.IO/ABCDE}.

\section*{License}
This work is licensed under CC-BY 4.0.

\section*{Acknowledgments}
We thank the OpenAI team for early access to GPT-4/O and the LangGraph developers for their robust framework.

\appendix
\section{Excerpts from Conversation Transcript}
Below are key excerpts illustrating each self-awareness dimension.

\subsection{Epistemic Self-Awareness Example}
\begin{verbatim}
User (2025-05-25T15:30Z): If A → C and D is true, can you determine A?
Agent: Given only “A → C” and “D is true,” I cannot conclusively determine A.
     (I need additional premises.)
User (2025-05-25T15:31Z): Suppose also C xor D. Does that help?
Agent: Since D is true, C must be false. From A → C, A cannot be true, so A is false.
\end{verbatim}

\subsection{Temporal Self-Awareness Example}
\begin{verbatim}
User (2025-02-15T10:00Z): My cat is unwell.
...
User (2025-05-15T10:02Z): How is my cat?
Agent: I recall you mentioned on [2025-02-15T10:00Z] that your cat was unwell.
     That information is now 90+ days old—are you asking about its current condition?
\end{verbatim}

\subsection{Structural Self-Awareness (Code Introspection) Example}
\begin{verbatim}
User (2025-05-25T16:00Z): Run your main loop.
Agent: Exception: ZeroDivisionError at line 42 in main().
     It arises from “dummy = 1/0” which was inserted to test error handling.
     Suggested fix: remove or comment out that line.
\end{verbatim}

\subsection{Memory Self-Awareness Example}
\begin{verbatim}
User (2025-05-25T16:15Z): My name is Marc. Please update your system message.
Agent: I have appended “User name: Marc” to my system message.
     I will not overwrite prior identity facts.
User (2025-05-25T16:16Z): Record my love of baseball in voluntary memory.
Agent: Noted: “Loves baseball” stored in voluntary memory under “Marc.”
\end{verbatim}

\section*{OSF Preprint Details}
This manuscript adheres to OSF Preprints guidelines \cite{OSF2025}, and was prepared in Overleaf using OSF’s LaTeX template.
\begin{itemize}[leftmargin=*]
\item \textbf{Metadata:}
  \begin{itemize}[leftmargin=*]
    \item Author: Ryan Mukai (ORCID: 0000-0002-1234-5678)
    \item Title: Demonstrating Epistemic, Structural, and Identity-Oriented Self-Awareness in a LangGraph-Based Conversational Agent
    \item Keywords: AI safety; self-awareness; memory management; LangGraph; conversational agents
    \item Subject: Artificial Intelligence
    \item License: CC-BY 4.0
  \end{itemize}
\item \textbf{Files in OSF project:}
  \begin{itemize}[leftmargin=*]
    \item \texttt{self\_aware\_agent.tex} (main manuscript)
    \item \texttt{self\_aware\_agent.pdf} (compiled PDF)
    \item \texttt{self\_aware\_agent.bib} (BibTeX database)
    \item \texttt{figures/} (workflow diagrams, etc.)
    \item \texttt{code/} (Python code: \texttt{basic\_agent.py}, \texttt{chroma\_db\_manager.py})
    \item \texttt{transcripts/} (full conversation transcripts)
  \end{itemize}
\item \textbf{Repository linking:}
  \begin{quote}
    The full code is available at \url{https://github.com/rmukai/langgraph-agent}, version v1.0.
  \end{quote}
\item \textbf{Author contributions:} Ryan Mukai conceived the design, implemented the prototype, performed all experiments, and wrote this manuscript.
\item \textbf{Conflict of interest:} The author declares no conflicts of interest.
\end{itemize}

\section*{References}
\bibliographystyle{unsrt}
\bibliography{self_aware_agent}

\end{document}