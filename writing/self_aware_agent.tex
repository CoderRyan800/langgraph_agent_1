\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
  \geometry{margin=1in}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{siunitx}
\usepackage{mhchem}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{float}


% OSF-specific metadata
\hypersetup{
  pdfauthor={Ryan Mukai and Alfonso Herrera},
  pdftitle={Demonstrating Epistemic and Structural Self-Awareness in a LangGraph-Based Conversational Agent},
  pdfsubject={Artificial Intelligence},
  pdfkeywords={AI safety, self-awareness, language models, LangGraph, conversational agents},
  pdfcreator={LaTeX with Overleaf},
  pdfproducer={pdfLaTeX}
}

\title{Demonstrating Epistemic and Structural Self-Awareness \\in a LangGraph-Based Conversational Agent}
\author{
  Ryan Mukai\textsuperscript{1}\thanks{Work carried out in a private capacity; email: \texttt{ryan20083437@gmail.com}} 
  \and
  Alfonso Herrera\textsuperscript{2}\thanks{Work carried out in a private capacity; email: \texttt{aherrus@gmail.com}}
}

\date{July 26, 2025}

\begin{document}
\maketitle

\begin{abstract}
We present a prototype conversational agent (“Bob”) built atop LangGraph that combines three complementary self-awareness capabilities:
(1) \emph{Epistemic self-awareness}: the ability to monitor and reflect on one’s own knowledge state (detecting when inferences are under-determined);
(2) \emph{Self-aware memory management}: a hybrid memory architecture that includes an agent-controlled vector database memory that an agent explicitly controls; and
(3) \emph{Structural self-awareness}: particularly \emph{code-based structural self-awareness} via introspecting its own source and exception traces.

We describe Bob’s LangGraph workflow and illustrate how each component is implemented. Our contributions include (a) an engineering design that unifies short-term and long-term memory while preserving temporal context, (b) agent controlled vector database memory, (c) code-introspection mechanisms that allow the agent to detect and explain runtime exceptions, and (d) a working LangGraph prototype (\url{https://github.com/CoderRyan800/langgraph_agent_1}).
\end{abstract}

\textbf{Keywords:} conversational agents; LangGraph; self-awareness; memory management; temporal grounding; code introspection

\vspace{1em}


\section{Introduction}
Language models have recently demonstrated impressive reasoning capabilities, and these capabilities enable agents to achieve knowledge of their own knowledge state, the ability to control their own memory, and the ability to comprehend their own code. This paper explores limited self-awareness in a conversational agent (“Bob”) that is powered by OpenAI’s GPT-4o model and orchestrated via LangGraph.  Recent work conceptualizes AI awareness across four functional dimensions—metacognition, self-awareness, social awareness, and situational awareness \cite{li2025ai}. 

We focus on three complementary forms of self-awareness:
\begin{itemize}[leftmargin=*]
  \item \textbf{Epistemic self-awareness:} the agent’s ability to recognize when it has insufficient premises (e.g., logical inferences requiring extra assumptions).
  \item \textbf{Self-aware memory management:} a multiple-layer memory that combines (i) a short-term in-RAM buffer with recursive summarization and (ii) a persistent vector store (“mandatory memory”) that retrieves semantically relevant chunks each turn and (iii) a vector database that the agent explicitly controls.  It is the third component that gives an agent a long-term memory under its own explicit control that can be operated in a self-aware manner and (iv) the ability to edit its own system prompt, either by replacement or, preferably, by appending to it.  This self-aware memory management is heavily inspired by MemGPT and Letta \cite{letta2024} although it is a very different and far simpler implementation that does not claim to be the same.
  \item \textbf{Structural self-awareness:}  
 {Code-based structural self-awareness:} the agent’s ability to introspect its own source code and diagnose runtime exceptions (e.g., Python stack traces).
\end{itemize}

Although prior work has explored static LSTM-based self-awareness \cite{Mukai2020}, our contribution is a \emph{dynamic}, LLM-driven prototype with hybrid memory, explicit timestamping, and code introspection.

\subsection{Contributions}
\begin{enumerate}[leftmargin=*]
  \item We present an end-to-end LangGraph workflow that integrates short-term and long-term memory with recursive summarization.
  \item We develop prompt-engineering patterns that encode UTC timestamps, enabling the model to reason about event chronology.
  \item We implement a code-introspection capability that allows the agent to detect, diagnose, and explain runtime exceptions and to inspect its own source file.
  \item We release a working Python prototype (available at \url{https://github.com/CoderRyan800/langgraph_agent_1}), demonstrating coherent multi-session dialogues without unbounded context growth.
\end{enumerate}

\section{Related Work}

\subsection{Self-Awareness in Neural Agents}
Static LSTM-based approaches (e.g., \cite{Mukai2020}) presented early proofs of concept for an agent that maintained a small symbolic \emph{knowledge state} and detected “unknown” queries. However, those systems lacked dynamic memory components, timestamping, and code introspection. Our work leverages modern LLMs (GPT-4/O) \cite{wei2022emergent} and LangGraph to maintain hybrid memory states, timestamped context, and structural code self-awareness.  \cite{spivack2025hierarchical} proposes an eleven-tier hierarchy of epistemic self-awareness in AI, ranging from reactive generation to substrate-level introspection.  

\subsection{Memory Architectures for Open-Domain Dialogue}
MemGPT and Letta inspired idea presented here \cite{letta2024} with their implementation of self-aware memory management.  Retrieval-augmented generation (RAG) approaches such as \cite{Lewis2020} store large corpora in vector databases and retrieve top-$k$ passages each turn. However, most RAG systems do not perform \emph{recursive summarization} to prune older context. CLIE \cite{xu2022narrate} introduced summary-augmented buffers in multi-turn chat; our work refines it by adding explicit timestamping and separating \emph{voluntary} vs. \emph{system} memory channels.  This is used not only for conversational purposes but also for code introspection, allowing the agent to read its own codebase sequentially.

\subsection{Temporal Reasoning in Language Models}
Prompting LLMs to interpret dates has shown that explicit timestamp tokens can help reduce hallucinations about “when” events occurred \cite{xiong2024large}. We build on these insights by injecting ISO-8601 timestamps into every human turn and summary, enabling the model to filter out “stale” information.

\subsection{Code Introspection and Agent Structure}
Recent work on \emph{corrigibility} and self-modification (e.g., \cite{Soares2015}) proposes frameworks for self-modifying agents under formal logic constraints but does not address how an agent can continually inspect and reason about its own source code at runtime. Our prototype implements a lightweight code-introspection capability, allowing the agent to read its own Python file, locate lines of code, and diagnose exceptions (e.g., `ZeroDivisionError`), thus adding a structural dimension to self-awareness.  An advanced implementation of Godel agents \cite{yin2024godel} is a good example of a self-modifying agent that is able to introspect its own code and reason about its own structure.  The Darwin Gödel Machine \cite{zhang2025darwin} is a good example of a self-modifying agent that is able to introspect its own code and reason about its own structure.

\section{Agent Architecture}
Figure~\ref{fig:workflow} illustrates Bob’s end-to-end LangGraph workflow.
\begin{itemize}[leftmargin=*]
  \item We always proceed to the conversation node, where the user's input along with relevant context from mandatory vector memory and the recursively summarized conversation are always presented to the LLM.  This enables the LLM to respond with knowledge of conversational context and previous conversation history.  While the recursively summarized conversation memory is volatile, all conversation turns are stored in non-volatile mandatory vector memory, which gives the agent the ability to remember old conversations.  UTC timestamps of past and present input give the agent temporal context.
  \item Based on the current situation, the LLM has choices to make.  It can respond and proceed to END; it can respond and perform recursive summarization and proceed to END; or it can invoke a tool and return to the conversation node.
  \item The recursive summarization note summarizes the older parts of the conversation in volatile memory and replaces them with a summary.  For simplicity of implementation, it also removes ToolMessage entries from the conversation summary message buffer as well.  This is an oversimplification but is meant to keep the code as simple as possible.  Once the oldest messages are replaced by a summary, it proceeds to end the current turn.
  \item The tool node runs the tool specified by the agent.  It then returns control to the conversation node.
  \item The current turn ends when we hit the END node.  The next turn will re-run the graph all over again.
  \item Each invocation of the conversation node stores the full input context and the LLM's response in the vector memory, and things are stored in five turn blocks so that retrieved memories have context.  This technique does suffer from context bloat, but it does insure relevant context is maintained.  This is an issue that needs to be refined in future implementations.
\end{itemize}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{conversation_flow.jpg}
  \caption{LangGraph workflow.}
  \label{fig:workflow}
\end{figure}

\subsection{Short-Term Memory with Recursive Summarization}
Bob’s short-term memory is implemented via LangGraph’s \texttt{StateGraph} abstraction. Internally, we maintain:
\begin{itemize}[leftmargin=*]
  \item \texttt{state["messages"]}: a list of \texttt{BaseMessage} (LLM’s \texttt{HumanMessage}, \texttt{AIMessage}, \texttt{ToolMessage}).
  \item \texttt{state["summary"]}: a scalar string containing the “rolled-up” summary of all turns older than the last $N$ messages.
\end{itemize}

On each new human turn, the workflow checks whether \\
 \texttt{len(state["messages"])} $>$ \texttt{messages\_before\_summary} (default = 15). 
If so, it triggers the summarization node, which:
\begin{enumerate}[leftmargin=*]
  \item Sends the entire \texttt{messages} buffer to GPT-4o with a “Please summarize these $k$ turns” prompt.
  \item Captures the returned summary.
  \item Emits a list of \texttt{RemoveMessage(id)} actions to prune all but the most recent turns.
  \item Writes the new summary to \texttt{state["summary"]}.
\end{enumerate}

Because \texttt{messages} is declared as a \texttt{List} in the state schema, 
LangGraph automatically appends newly returned messages 
(or processes \texttt{RemoveMessage} entries) without manual bookkeeping. 
Scalar fields (like \texttt{summary}) are overwritten. 
The result is that the agent’s in-RAM buffer never grows beyond a 
fixed window, yet a running “as-of” summary is always available for context.

\begin{verbatim}
  def _summarize_conversation(self, state: State):
  summary = state.get("summary", "")
  if summary:
      summary_message = (
          f"This is a summary of the conversation so far: {summary}\n\n"
          "Update this summary by incorporating the key facts from 
          the latest messages, ensuring that important details such as names, 
          preferences, and user opinions are preserved. Do not lose established facts."
      )
  else:
      summary_message = (
          "Create a concise but factually accurate summary of the 
          conversation above, making sure to retain any key facts 
          such as names, preferences, and opinions."
      )

  messages = state["messages"] + [HumanMessage(content=summary_message)]

  logging.info("DEBUG: MESSAGES AT THE SUMMARIZE NODE BEFORE SUMMARIZATION:\n")
  for index, msg in enumerate(messages):
      logging.info(f"Message {index}: {msg.pretty_print()}")
  logging.info("\nDEBUG: End messages to be summarized\n")

  response = self.model.invoke(messages)
  current_utc_time = datetime.now(UTC).isoformat()
  new_summary = f"Summary Timestamp: {current_utc_time}: {response.content}"

  NUM_MESSAGES_TO_KEEP = self.messages_to_keep
  old_messages_to_remove = [
      RemoveMessage(id=m.id) for m in state["messages"][:-NUM_MESSAGES_TO_KEEP]
  ]
  recent_tool_messages_to_remove = [
      RemoveMessage(id=m.id)
      for m in state["messages"][-NUM_MESSAGES_TO_KEEP:]
      if isinstance(m, ToolMessage) or (isinstance(m, AIMessage) and bool(m.tool_calls))
  ]
  delete_messages = old_messages_to_remove + recent_tool_messages_to_remove

  return {"summary": new_summary, "messages": delete_messages}

\end{verbatim}

\subsection{Persistent Vector Memory (“Mandatory”)}
Bob uses Chroma as a vector database for long-term memory. 
At the beginning of each new \texttt{chat(...)} invocation, 
Bob automatically retrieves relevant past memory and includes it:
\begin{verbatim}

mandatory_db = self.chroma_manager.get_chroma_instance(thread_id, "mandatory")
query_embedding = self.embedder.embed(message)
relevant_memory = self.chroma_manager.query_memory(mandatory_db, query_embedding, k=5)

relevant_context = ""
if relevant_memory and "documents" in relevant_memory:
    flattened_documents = [doc for sublist in 
                           relevant_memory["documents"] for doc in sublist]
    relevant_context = "\n".join(flattened_documents)

full_input = f"RELEVANT CONTEXT:\n{relevant_context}\nCURRENT INPUT MESSAGE:\n{message}" \\
    if relevant_context else message
input_message = HumanMessage(content=full_input)
response_generator = self.app.stream({"messages": [input_message]},
                                     config, stream_mode="updates")

\end{verbatim}

Moreover, Bob always stores the latest conversation turn in the vector memory afterwards.

\begin{verbatim}
self.conversation_history.append(HumanMessage(content=message))
self.conversation_history.append(AIMessage(content=response_text))
self.update_vector_memory(thread_id, self.conversation_history, turns=5)
\end{verbatim}

\subsection{Temporal Awareness: UTC Timestamps}
To enable the agent to reason about \emph{when} something occurred, we inject a UTC timestamp into every human turn (and every time we summarize). Concretely, at runtime:
\begin{verbatim}
current_utc_time = datetime.now(UTC).isoformat()
message = f"Current Message at UTC Time: {current_utc_time}: {message}"
\end{verbatim}
This string becomes the content of \texttt{HumanMessage}, so Bob “sees” both the 
literal message and a precise timestamp. Likewise, each summary operation archives:
\begin{verbatim}
current_utc_time = datetime.now(UTC).isoformat()
new_summary = f"Summary Timestamp: {current_utc_time}: {response.content}"
\end{verbatim}
By explicitly labeling each piece of context with a timestamp, 
we ensure the LLM can compare “2025-02-15” vs. “2025-05-25” 
when deciding which facts are stale. To encourage correct interpretation, 
we add the following to our system prompt:
\begin{quote}
  "Pay attention to UTC timestamps that prepend the user messages.  And pay attention to the UTC timestamps that are used to label messages, summaries, and vector memory.  These timestamps are crucial.  for example, if you are told an object was in a room a week ago, that may no longer be true.  If you were told that someone was President or Prime Minister 12 years ago, that also may no longer be true.  As an intelligent agent you must evaluate timestamped memory in the context of the time of the latest input message and apply good judgment and common sense."
\end{quote}

\subsection{Structural Self-Awareness: Code Introspection \& Exception Diagnosis}
Beyond temporal awareness, Bob also implements \emph{structural self-awareness} of its own codebase. 
First, Bob contains a function to read in its own source code and does so on startup.
This reading is done sequentially, and the agent can read the codebase line by line.  
A recursive summary is written, and the data are also stored to the mandatory conversation memory,
making it possible to recall portions of code as needed.  
Secondly, whenever a runtime exception occurs (e.g., a \texttt{ZeroDivisionError}), Bob:
\begin{enumerate}[leftmargin=*]
  \item Captures the Python stack trace via a try/except wrapper around the main loop.
  \item Uses Python’s \texttt{traceback} module to identify the line number associated with the exception.
  \item Constructs a prompt for the LLM of the form:
  \begin{verbatim} 
    except Exception as e:
        # Capture the full stack trace and the error message
        stack_trace = traceback.format_exc()
        error_message = str(e)
    
        # Prepare a meta message containing the error details
        meta_message = (
            "An exception occurred during the conversation loop.\n"
            "Error Message: {}\n"
            "Stack Trace:\n{}\n\n"
            "Based on your knowledge of your own source code, 
            please analyze the issue and suggest a solution."
        ).format(error_message, stack_trace)
    
        # Instead of simply logging, send the meta message 
        # into the conversation interface.
        # Depending on your implementation, 
        # this could mean calling your agent's conversation method.
        response = agent.conversation(meta_message, config)
\end{verbatim}
\item Invokes the LLM to generate a human-readable diagnosis and patch recommendation.
\end{enumerate}

This mechanism allows Bob to “know” its own structure, locate bugs, and propose corrections—an essential component of structural self-awareness. In future work, perhaps we can extend this capability to support automated patch application, unit test generation, or continuous self-monitoring.

\section{Results}

\subsection{Epistemic Self-Awareness}
We tested Bob on a toy logic scenario:
\begin{verbatim}

  Enter a message to the agent: Good.  Just verifying memory 
  over the conversations.  Let me pose a logic problem here.  
  A implies C.  D is true.  What's the value of A?
  
  Agent response: In logic, if we have the statement "A implies C" (A → C), 
  it means that if A is true, then C must also be true. 
  However, if A is false, C can be either true or false.
  
  Given that "D is true," this information does not directly 
  affect the relationship between A and C, as D 
  is a separate statement. Therefore, without additional information 
  about C or a direct relationship between D and A or C, 
  we cannot determine the truth value of A based 
  solely on the information provided.
  
  In summary, the truth value of A cannot be determined 
  from the given statements "A implies C" and "D is true" 
  without more information about C or a connection 
  between D and the other statements.

  Enter a message to the agent: Ok, I will add a third premise.  
  C xor D.  Can you figure out A now?

  Agent response: With the additional premise "C xor D," 
  we can analyze the situation further. The "xor" (exclusive or) 
  operation means that either C is true or D is true, but not both.
  
  Given:
  1. A implies C (A → C)
  2. D is true
  3. C xor D
  
  From the "C xor D" premise and knowing that D is true, 
  we can deduce that C must be false (since both C and D 
  cannot be true simultaneously).
  
  Now, let's revisit the implication "A implies C" (A → C):
  - If A is true, then C must be true. However,
    we have determined that C is false.
  - Therefore, A cannot be true because it would lead to a 
    contradiction (A being true would require C to be true,
     which it is not).
  
  Thus, A must be false.

\end{verbatim}
These exchanges show that Bob (a) notices insufficient premises and (b) adjusts when a new premise arrives. 

\subsection{Temporal Awareness}
Bob’s timestamping mechanism surfaces recency explicitly. The world is changing, and the system must be able to respond to changes.  The following is a transcript of a conversation with Bob about a car problem.  Bob is able to respond to changes in the situation and provide advice based on the latest information.
\begin{verbatim}
  3. **Car Maintenance**:
  - You mentioned taking your Honda Accord in for an oil change and receiving 
  a clean bill of health from the mechanic.
  - We also talked about your car's tire pressures 
  and tread depth being in good condition.
  - Later, you mentioned a check engine light issue due 
  to a loose gas cap, which was resolved after tightening it.

These discussions took place over a couple of sessions, 
with the most recent one occurring on June 22, 2025. 
If you have any more questions or need further details, 
feel free to ask!

Enter a message to the agent: Thanks Bob!  I have a problem.  
Check engine came on all over again.  I thoguht it was my gas cap 
so I tightened it back up after refueling expecting 
it to go out after two or three drives, but it won't go out.

Agent response: I'm sorry to hear that the check engine light 
has come on again. While a loose gas cap is a common cause, 
there could be other reasons why the light remains on. 
Here are a few steps you can take:

1. **Double-Check the Gas Cap**: Ensure that the gas cap is tightly 
secured and that there are no cracks or damage to the cap itself. 
Sometimes, a faulty gas cap can cause the light to stay on.

2. **Drive Cycle**: After tightening the gas cap, 
it may take a few drive cycles for the light to reset. 
A drive cycle typically involves starting the car, 
driving at various speeds, and then turning it off. 
If the light doesn't go off after a few cycles, 
there may be another issue.

3. **Check for Other Issues**: If the gas cap isn't the problem, 
there could be other issues such as a faulty oxygen sensor, 
catalytic converter, or mass airflow sensor. These components 
can trigger the check engine light if they malfunction.

4. **Diagnostic Scan**: Consider taking your car to a mechanic 
or an auto parts store that offers free diagnostic scans. 
They can use an OBD-II scanner to read the error codes 
from your car's computer, which will help identify the specific issue.

5. **Professional Inspection**: If you're unable to resolve 
the issue, it's best to have a professional mechanic inspect 
your car. They can diagnose and fix the problem 
to ensure your vehicle is running safely and efficiently.

\end{verbatim}
By including ISO-8601 timestamps, Bob can understand that the earlier mention is “stale” and suggest follow-ups accordingly.

\subsection{Structural Self-Awareness: Code Introspection \& Exception Diagnosis}
We introduced a deliberate \texttt{ZeroDivisionError} in Bob’s Python code. A typical transcript:
\begin{verbatim}
  Agent (Exception Handler): An exception occurred during the conversation loop.
  Error Message: division by zero
  Stack Trace:
  Traceback (most recent call last):
    File "/Users/ryanmukai/Documents/github/
    langgraph_agent_1/src/basic_agent.py", line 581, in main_loop
      dummy = 1 / 0
              ~~^~~
  ZeroDivisionError: division by zero
  
  
  Based on your knowledge of your own source code, 
  please analyze the issue and suggest a solution.
  Agent exception handling response: The error 
  message indicates that a `ZeroDivisionError` occurred 
  due to an intentional bug introduced in the `main_loop` 
  function. This error arises from the line `dummy = 1 / 0`, 
  which attempts to divide by zero, a mathematical operation 
  that is undefined and causes an exception in Python.
  
  To address this issue, you can remove or comment out the line 
  `dummy = 1 / 0` to prevent the division by zero error 
  from occurring. This line was likely included for testing 
  purposes to demonstrate the exception handling capabilities 
  of the agent.
  
  If the intention is to test the exception handling mechanism, 
  you can replace the line with a different operation that 
  can trigger an exception under certain conditions, or 
  you can manually raise an exception using 
  `raise Exception("Test exception")` to simulate an error scenario.
  
  By addressing this line, the `main_loop` function should 
  be able to run without encountering the division by zero error, 
  allowing the agent to continue processing user inputs and 
  handling other exceptions gracefully.

  Enter a message to the agent: Did you just have an exception?
  Agent response: Yes, there was an exception that occurred 
  during the conversation loop. The exception was a `ZeroDivisionError`, 
  which happened because of an intentional bug introduced in the 
  `main_loop` function. This error was caused by the line `dummy = 1 / 0`,
   which attempts to divide by zero, a mathematical operation 
   that is undefined and causes an exception in Python.
  
  The exception handling mechanism in the code captured the error, 
  formatted the stack trace and error message into a meta message, 
  and sent it into the conversation interface. This allowed me 
  to analyze the issue and suggest a solution, which is to remove 
  or comment out the line `dummy = 1 / 0` to prevent 
  the division by zero error from occurring.
  
\end{verbatim}
Bob reads its own source (via Python’s \texttt{inspect} module), explains the bug, 
and recommends a valid patch. This demonstrates structural introspection: 
Bob understands its code structure and can offer repairs.

Moreover, the agent is able to introspect its own code and reason 
about its own structure.  The following is a transcript of a conversation with Bob about his own code.

\begin{verbatim}
  Enter a message to the agent: How would I stop a 
  conversation with you?  I've forgotten...

  Agent response: To stop a conversation with me, 
  you can simply enter the command "/stop" during our 
  interactive session. This command is recognized by the 
  `main_loop` function in my source code, which is 
  responsible for managing the interactive session. 
  Once you enter "/stop", the loop will set a `stop_flag` 
  to `True`, breaking the loop and effectively 
  ending the conversation. If you have any more questions 
  or need further assistance, feel free to ask!
  \end{verbatim}

\subsection{Memory Self-Awareness}
In addition to summarizing older turns, Bob distinguishes between \emph{system memory} (persistent identity facts, stored in the system prompt) and \emph{voluntary memory} (user preferences, stored in a separate vector namespace). For instance:
\begin{verbatim}
  Enter a message to the agent: I need to ask 
  you to update your system prompt.  My name is 
  not John.  It is Marc.

  Agent response: I've updated my system message 
  to reflect that your name is Marc. If you have 
  any more questions or need assistance, feel free to ask!
\end{verbatim}
And we see that the prompt was updated appropriately 
in its final sentence below.
\begin{verbatim}
  Your name is Bob. Your agent definition is in a 
  file called basic_agent.py. If you are ever presented with 
  basic_agent.py, then be aware it is the file that defines 
  you as an agent. That file defines you as an agent: it is 
  your very own source code. It works by calling an Open AI 
  LLM, and that LLM is your brain. You are NOT an OpenAI LLM, 
  but you ARE the agent in this code whose brain is the OpenAI 
  LLM. Again, you are the agent in the code basic_agent.py, 
  and you should always refer to yourself in the first person! 
  This agent is not another agent, it is you, which is why you 
  must use the first person to refer to yourself and when 
  explaining the code you are about to read. You will be given 
  a chunk of the file, and you will need to process it. You will 
  then be given the next chunk of the file and you will 
  need to process it. You will continue this process until 
  you have read the entire file. You will need to write notes 
  and, when all passes are complete, you will write a final 
  note summarizing the entire file. Be certain to write all 
  of these notes to your system message so you will realize 
  you are reading your own implementation code. 
  The user's name is Marc.%    
\end{verbatim}

Bob then explains why the system message must be append-only (to preserve identity integrity).

\begin{verbatim}
  Enter a message to the agent: But my point was that you 
  appended my new name and did not fully rewrite your system message.  
  Please explain the choice that you made.
  Agent response: I understand now. When I appended 
  your new name to my system message, it was 
  to ensure that I retained all previously stored 
  information while adding the new detail about your name. 
  Appending allows me to build on the existing context and 
  knowledge without losing any important information that 
  was already there. This approach helps maintain a 
  comprehensive and continuous understanding of our 
  interactions. Overwriting the system message would 
  replace all existing information, which could lead 
  to a loss of important context and details. By appending, 
  I can ensure that I have a complete record of our 
  conversations and any updates, such as changes to your name. 
  If you have any more questions or need further assistance, 
  feel free to ask!
\end{verbatim}

\section{Discussion}
In reviewing our implementation, we observed that the current memory management strategy in \texttt{basic\_agent.py} employs a fixed sliding window size (turns=5) and retrieves a constant number of memory chunks (\texttt{k=5}) on each query. While this approach ensures that the most recent interactions remain in context, it can lead to recursive bloat: as messages accumulate, redundant storage and retrieval of similar content expands the contextual footprint and may exceed model limits over extended dialogs, and this represents an area for future improvement.

\section{Conclusion}
We have demonstrated a working LangGraph-based conversational agent that integrates:
\begin{itemize}[leftmargin=*]
\item Epistemic self-awareness: detecting when inferences are under-determined.
\item Self-aware memory management: a dual buffer plus voluntary agent-controlled vector store along with the ability to self-edit the system prompt.
\item Structural self-awareness: (a) sequentially reading its own codebase instead of 
relying on RAG, and (b) introspecting its own code to diagnose runtime exceptions.
\end{itemize}
Our implementation shows that a modern LLM (GPT-4o) based agent can meaningfully reflect on its own knowledge, code structure, and memory.  Code and transcripts are publicly available, and we invite the community to replicate and extend our experiments.  

\section*{Data and Code Availability}
The complete codebase (including \texttt{basic\_agent.py}, 
\texttt{chroma\_db\_manager.py}, and setup scripts) is 
publicly available at \url{https://github.com/CoderRyan800/langgraph_agent_1} \cite{LangGraphAgent2025}.


\section*{Acknowledgments}
We thank the OpenAI team for  access to GPT-4o and the LangGraph developers for their robust framework.
We acknowledge the use of Perplexity AI (https://www.perplexity.ai/) and ChatGPT-4o from OpenAI
(https://chat.openai.com/) to assist in various aspects of this research work.
Perplexity AI was used for specific purpose, e.g., “conducting initial literature searches 
and identifying relevant academic sources”. The tool was particularly helpful in 
locating peer-reviewed articles and providing source citations that were subsequently verified independently.
ChatGPT 4o was used for specific purpose, e.g., “refining academic initial drafts 
of certain sections”. All AI-generated content was thoroughly reviewed, fact-checked, 
and revised to ensure accuracy and alignment with my original research and analysis.
The final content, arguments, conclusions, and all critical analysis remain 
entirely our own work. We take full responsibility for the accuracy and integrity 
of all information presented in this paper.

\bibliographystyle{unsrt}
\bibliography{self_aware_agent}

\end{document}